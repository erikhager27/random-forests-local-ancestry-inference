[["index.html", "Random Forests and Local Ancestry Inference Chapter 1 Welcome to our eBook!", " Random Forests and Local Ancestry Inference Erik Hager, Brian Lee Chapter 1 Welcome to our eBook! This bookdown document serves as a digital artifact of our final presentation on Random Forests and Local Ancestry Inference as part of our Statistical Genetics capstone course at Macalester College. Through starting with some basic genetics, an introduction of genome wide association studies and local ancestry inference, we plan to discuss our intentions and motivations in Chapter 2. In Chapter 3, we discuss the building blocks of random forests – decision trees. Subsequently, building off of this discussion, we delve into bootstrap aggregation and random forests. In our final section, we hope to unify our project by applying the material discussed in previous chapters to a small case study using toy data. "],["introduction-to-gwas-ancestry-and-local-ancestry-inference.html", "Chapter 2 Introduction to GWAS, Ancestry, and Local Ancestry Inference 2.1 Basic Genetics 2.2 GWAS and Why Ancestry Matters 2.3 Project Motivations &amp; Local Ancestry Inference", " Chapter 2 Introduction to GWAS, Ancestry, and Local Ancestry Inference 2.1 Basic Genetics The human genome consists of 23 pairs of chromosomes, each of which contains millions of nucleotide base pairs joined together in a double helix. The nucleotides adenine (A) and thymine (T) always pair with each other, as do cytosine (C) and guanine (G). At any point along a person’s DNA strand, the sequence of A’s, T’s, C’s, and G’s dictates characteristics of that person. When comparing any two human genomes, the entire strands of nucleotides will be about 99% identical. However, there are small portions of the genome where the sequences vary from person to person, and these places are called genetic variants. Different nucleotide sequences in these areas are what cause variation in the human population. Figure 1: This image depicts segments of two copies of a person’s chromosome, representing one received from each parent. The outlined section represents a SNV, showing a single position along the strands where different alleles (A/T vs. G/C) were received from the two parents. (Source: Kelsey Grinde) When humans are conceived, they receive one copy of all 23 chromosomes from their mother, and one copy from their father, as depicted in Figure 1. Thus, when considering a single-nucleotide genetic variant (SNV) at a particular position on any chromosome, it is the case that a person will have received one allele (A/T, G/C, etc.) from each parent at that position. These two alleles together carry the meaning of that SNV, potentially in the form of an observable trait. Most SNVs are considered biallelic, meaning only two of the four total base pair combinations are possible to be inherited as alleles at a SNV’s particular genomic position. The pair that occurs more commonly across the human population at a SNV is referred to as the major allele, and the less common pair is referred to as the minor allele. Whether a person received 0, 1, or 2 minor alleles from their parents at a given position determines their minor allele count (MAC) at that spot. If the overall frequency of minor alleles for a given SNV is greater than 1% across the human population, it may instead be referred to as a single-nucleotide polymorphism (SNP). It is data on individuals’ MACs at SNPs in particular that is commonly used as the backbone of genetic studies. 2.2 GWAS and Why Ancestry Matters In order to learn about how various SNPs are linked to observable human traits, geneticists often conduct what are called genome-wide association studies (GWAS). These studies involve gathering genetic and observational data on many individuals, some of whom may possess a certain trait of interest. After acquiring the exact sequence of subjects’ genomes, experimenters sift through the sequences for SNPs and record subjects’ MACs at each one. Then, regression analysis can be performed to find correlations between individuals’ MACs at various SNPs and their observed traits. This helps experimenters draw conclusions about which SNPs are universally important in determining those traits. Ancestry, however, can often be a confounding factor that muddles the true relationship between SNPs and observed traits during GWAS, as depicted in Figure 2. Consider, for example, a study attempting to determine which SNPs cause brown hair, but without accounting for German ancestry as a possible confounder. A genotype with SNPs resulting from a German pedigree might show a strong correlation with brown hair if people of German descent are more likely to have brown hair in general. However, concluding that those particular SNPs cause brown hair would be erroneous, since it was really the fact that the people with those certain SNP characteristics were of German origin that caused them to have brown hair. Thus, gathering information about subjects’ ancestries is a vital part of GWAS, in that including such data in the regression models helps correct for this type of confounding effect. Figure 2: This is a DAG depicting the confounding effect of ancestry in GWAS. Doubt can be cast on a potential relationship between a genotype and a trait if ancestry has a true causal relationship with both. 2.3 Project Motivations &amp; Local Ancestry Inference Our Statistical Genetics course has provided us with a diverse sampling of methods used for genome wide association studies. For our project, we wanted to use this tool set of methodologies to expand on themes of ancestry and classification by honing in on local ancestry inference (LAI). That is, we wanted to see if we could infer the ancestral origins of certain segments of an individual’s genome based on the presence, or absence, of genetic variants of interest. Figure 3: Local ancestry inference can allow researchers to find the allele ancestries at each locus of an individual’s genome, denoted by blocks of color in the genome following LAI. Within the realm of accomplishing the goals of a genome wide association test – to see if a trait of interest is associated with a certain genetic variant – local ancestry inference is especially helpful in identifying genetic variants associated with traits or diseases in admixed groups, or ethnic groups whose genomes have resulted from a recent mixture of two or more geographically distinct ancestral populations. In short, local ancestry is particularly useful in genetic analyses of diverse cohorts, in addition to being useful for admixture mapping. Some admixed populations of interest include African Americans with African and European Ancestry and Latino Americans who have African, European, and Native American ancestry. In summary, genetic admixture mapping using LAI has interdisciplinary applications, as it can allow for a richer understanding of historical human migration to further corroborate current historical analyses using GWAS. "],["classification-decision-trees-the-building-blocks-of-random-forests.html", "Chapter 3 Classification Decision Trees: The building blocks of Random Forests 3.1 Introduction 3.2 Classification Tree Example", " Chapter 3 Classification Decision Trees: The building blocks of Random Forests 3.1 Introduction We’ve found that Random Forests provide an excellent basis for elucidating the role of certain SNPs in relation to inferring local ancestry. Before diving into random forests, we will discuss their building blocks: decision trees. Within the context of inferring local ancestry, we want to figure out how to categorize a certain part of an individual’s genome using predictors, which in this case are our genetic variants, SNPs. Below is a decision tree, labeled with its general components. Figure 4: Depiction of the structure of a decision tree, labeled with each component. Image sourced from https://www.javatpoint.com/machine-learning-decision-tree-classification-algorithm Our first initial node, or the root node, essentially takes our most influential predictor and classifies our data into subsets for the subsequent decision nodes. Each split in the data made at each decision node is representative of regions in the predictor space. That is, each split on a predictor results in regions of the predictor space. Each following decision node is made up of the next best predictive parameters. At the end of each decision node, the classification tree makes its prediction at the leaf nodes. In general, the strategy of a classification tree is to continuously split the data into subsets or regions based on predictor values until the regions are uniform in their outcomes. 3.2 Classification Tree Example To better grasp the idea of decision trees, below is an example a classification tree in R I’ve produced using code from Prof. Brianna Heggseth’s Statistical Machine Learning class site using the Urban Land Cover dataset from the UCI Machine Learning Repository which contains data collected on the observed type of land cover (determined by human eye) and “spectral, size, shape, and texture information” computed from a given image. Figure 5: Example classification tree The tree above was trained using data collected on textural and shape characteristics of photographs of land from our dataset, the codebook for which is available here. The variables themselves aren’t very pertinent, but they can be helpful to keep in mind when trying to see the metric by which the tree splits the data by. However, for ease of reference, here are the four predictors chosen by the decision tree above at each node: NDVI: Normalized Difference Vegetation Index (spectral variable) Bright_100: Brightness (spectral variable) SD_NIR: Standard deviation of Near Infrared (texture variable) GLCM2: Gray-Level Co-occurrence Matrix Take a look at the red, leftmost leaf node. If the condition at the branching point is true, we go left. If false, we go right. In this case, if NDVI is less than -0.01, the tree predicts that a case with such a value will be classified as asphalt. Notice that there are three lines of text within this node: Asphalt, which is the predicted class The predicted probability of the node’s classification: 97% for asphalt, 0.02 for grass and tree classifications respectively 22%, depicting the proportion of the cases from the dataset within this node. Also notice that as we follow the tree down to its leaves, the regions become more homogenous in the classes represented than the original regions before the split, as seen in the second line of text in each node. We can see that for each class of land type, the predicted probability of the node’s classification either increases closer to 1.0, or decreases closer to 0.0 with each split. This is what we know as node purity, quantified by what’s known as the Gini index, or the impurity measure (not to be confused with the economic measure of income inequality). When a node is pure, or when most of the cases belong to one class, the Gini index takes a small value. When a region is impure, or when cases in a given node belong to different classes, the Gini index takes a large value. At face value, we can see that trees are fairly easy to understand, and you may have seen or used a classification tree at some point in your life. This is one of the greatest advantages of trees – that they are generally intuitive and easy to explain. However, one of the biggest disadvantages to decision trees is that they are high in variance, and thus prone to overfitting. The cost of the simplicity of a tree is instability. Classification trees are known to be a greedy algorithm, meaning that locally optimal splits in the data are made at each node, thus reducing its capabilities to produce globally optimal classifications. In other words, one of the decision tree’s fundamental flaws is that it does not know when to stop growing, or when to stop making splits in the data, which can result in an overfitted tree. Figure 6: On the left is an example of an overfit tree. On the right is this tree, pruned. An over-specific decision tree takes away from its role as an algorithm by reducing its replicability/generalizability. With any algorithm, our goal is to take new data and run it through our algorithm which has been created and trained on existing data. The tree on the left of Fig. 5 is an example of what might happen if the tuning parameters are too loose – it will make decisions on splits in the data which are overfit to the data that was used to train it, thus poorly generalizing to new data. So, running a case through this tree will likely result in an inaccurate classification. On the right, is a “pruned” tree. The pruned tree provides looser parameters by which splits are made, resulting in a smaller tree. Though more generalizable, a smaller tree can fail to capture big-picture ideas from the dataset – small changes in the data can result in a large change in the final estimated tree. "],["bagging-and-random-forests.html", "Chapter 4 Bagging and Random Forests 4.1 Ensemble Learning Methods: Bagging 4.2 Random Forests", " Chapter 4 Bagging and Random Forests 4.1 Ensemble Learning Methods: Bagging We can mitigate the flaws of a decision tree by aggregating the results of a bunch of trees to narrow in on a decision that is globally optimal. This is essentially the idea behind ensemble learners. There are two main types of ensemble learners, but for the purposes of our project, we will be focusing on bagging, or Bootstrap AGGregatING. Figure 7: The idea of bootstrapping demonstrated. Image sourced from: https://online.stat.psu.edu/stat555/node/119/ As shown in figure 7, Bagging is an ensemble learning technique which helps improve the performance and accuracy of machine learning algorithms by employing a resampling technique known as bootstrapping. From our true population, we take a sample as an estimation of our population of interest. From there, rather than collecting many new samples from the population, we can randomly sample multiple subsets of our collected data to simulate the collection of multiple new samples from the population of interest. Through this process, we can build a model which produces results more similar to the true sampling distribution. Now, extending this idea, we can produce multiple weak, unpruned learners for each bootstrapped sample, combining the predictions made by each of these results to create a more powerful algorithm. 4.2 Random Forests Figure 8: Each individual tree has a different subset of predictors and thus differing classifications, both denoted by the varying colors of each decision node. This is the core idea behind random forests! Interestingly, the flaws of a decision tree is what makes the Random Forest such a strong learner – by using many overfit trees, each respectively trained with a different bootstrapped sample, we can aggregate these results to build a more robust model through taking the majority vote from all of the trees within the forest. We’ve discussed at length why a random forest algorithm is a forest, but what’s random about it? There are two things: By using many weak, overspecific decision trees, we can use the process of bagging to select a random bootstrap sample to be used in each tree Random forests use a randomly selected subset of predictors for each tree, meaning that rather than searching for the next best split from all the predictors, the tree must choose from the subset of predictors it has in its toolbox. Through these two important characteristics of random forests, we can decorrelate each tree within the Random Forest to mitigate the issues of high variance, greediness, and overfitting previously mentioned. "],["applications-of-random-forests-in-local-ancestry-inference.html", "Chapter 5 Applications of Random Forests in Local Ancestry Inference 5.1 Results", " Chapter 5 Applications of Random Forests in Local Ancestry Inference The random forest algorithm can be applied to genetic data to infer local ancestry by building and training a model on data where individuals’ ancestries are known. As shown in Figure 9, a data set can be constructed containing SNP MACs of various individuals known to come from different distinct ancestry populations. The SNP columns can be divided into local windows, each of which can be fed through the random forest model separately. The trees in the algorithm will ask questions about the MAFs of individuals in the given window, making splits based on the SNPs that demonstrate the greatest differences in MACs between people in the different ancestry groups. The many trees will each make splits based on these questions, and a bootstrapped aggregation of all of the trees will constitute a means for classifying any individual’s ancestry within a given window. This allows for a “painting” of each window of people’s genomes with the local ancestry inferred by the algorithm. Figure 9: This image depicts how genetic data with known ancestry is organized, as well as how decision trees are used for classifying these ancestries.(Source: Freddy Barragan) In order to demonstrate the algorithm in action, we constructed a toy dataset intended to represent one window of the genome of 1,000 individuals from two different ancestry populations, with 500 observations assigned to each group. To constitute our window, we used the rbinom() function in RStudio to generate MACs of 0 or 1 for 4 SNPs of interest and 26 noise SNPs, comprising a total of 30 variables. The SNPs of interest contained MACs generated from different rbinom() frequencies for people in each of our two populations, with the intention of these being important predictors for a decision tree classifying the populations to split on. The noise SNPs were generated using the same rbinom() frequency for all 1,000 observations, with the intention of these being variables that would hurt the accuracy of a tree forced to split on only a random subset of predictors. We then used the caret package in RStudio to train a random forest algorithm on our toy dataset. We used the default settings in caret to create a model with 500 trees to classify our ancestry populations, and tuned the variable subset size to range from 1 to 30 predictors. We set up the algorithm to select the model with the variable subset size that maximizes out of bag accuracy as our final choice. Our hope was that the model would correctly pick out our SNPs of interest as the best predictors to split upon, and that out of bag error estimates might give us an idea of how accurately a random forest model might classify something like local ancestry in the presence of noisy data. 5.1 Results Figure 10: This plot shows the out of bag accuracy of the random forest algorithm with variable subset sizes ranging from 1 to 30. The optimal fit is with 5-predictor subsets. The final model chosen by our random forest algorithm was one with a subset size of 5 predictors. As shown in Figure 10, a size of 5 predictors maximized the out of bag accuracy. This optimal subset size is what the model deems sufficiently large to ensure our SNPs of interest are considered often enough for splits, and also sufficiently small to ensure enough decorrelation of the trees before bagging. By contrast, the models tuned with less than 5 predictors would be considered underfit and high in bias, and those with more than 5 would be considered overfit to our data and high in variance. Figure 11: This table lists the top 10 SNPs in terms of split importance. It includes our 4 SNPs of interest labeled with s, and 6 noise SNPs labeled with t. Figure 12: This plot shows the mean decrease in the Gini coefficient achieved by each SNP as a predictor in our model. Figures 11 and 12 illustrate the importance of each SNP in our model as a split node, defined as the mean decrease in the Gini coefficient for each predictor across all 500 trees. As we had hoped, our 4 SNPs of interest stand out as far more important predictors to split upon than any of the 26 noise SNPs. Figure 13: This is the confusion matrix detailing how many of the 500 observations from each population our final model predicted correctly and incorrectly, as well as an overall error estimate. Figure 13 presents the overall performance of our final model. The confusion matrix tells us that this model produced an out of bag error rate of 21.1%, predicting 789 of the 1,000 total observations in our dataset correctly. Considering the inclusion of 26 noise SNPs in our window, our random forest algorithm did a relatively good job of predicting ancestry accurately. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
